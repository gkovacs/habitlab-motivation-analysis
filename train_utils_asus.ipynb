{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train_utils.ipynb to python\r\n"
     ]
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "!typech train_utils.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from memoize import memoize\n",
    "import copy\n",
    "import importlib\n",
    "from os import path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mkdata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getsecret import getsecret\n",
    "dataset_name = getsecret('DATA_DUMP') #'2019_04_01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter_info_list():\n",
    "  output = [\n",
    "    {\n",
    "      'name': 'dataset_name',\n",
    "      'type': 'dataset',\n",
    "      'values': [dataset_name],\n",
    "    },\n",
    "    {\n",
    "      'name': 'model_name',\n",
    "      'type': 'model',\n",
    "      'values': ['selfattentionlstm'],\n",
    "    },\n",
    "    {\n",
    "      'name': 'criterion',\n",
    "      'type': 'model',\n",
    "      'values': ['NLLLoss'],\n",
    "    },\n",
    "    {\n",
    "      'name': 'learning_rate',\n",
    "      'type': 'model',\n",
    "      'values': [0.00005, 0.005, 0.05, 0.0005],\n",
    "    },\n",
    "    {\n",
    "      'name': 'window_embed_size',\n",
    "      'type': 'model',\n",
    "      'values': [256, 64, 128, 512],\n",
    "    },\n",
    "    {\n",
    "      'name': 'difficulty',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'time_of_day',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'day_of_week',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'domain_productivity',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'domain_category',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'initial_difficulty',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'languages',\n",
    "      'type': 'feature',\n",
    "      'values': [True, False],\n",
    "    },\n",
    "    {\n",
    "      'name': 'num_prior_entries',\n",
    "      'type': 'dataparam',\n",
    "      'values': [10, 20, 30, 40],\n",
    "    },\n",
    "    {\n",
    "      'name': 'sample_every_n_visits',\n",
    "      'type': 'dataparam',\n",
    "      'values': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 32, 64, 128, 256, 512],\n",
    "    },\n",
    "    {\n",
    "      'name': 'sample_difficulty_every_n_visits',\n",
    "      'type': 'dataparam',\n",
    "      'values': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 32, 64, 128, 256, 512],\n",
    "    },\n",
    "    {\n",
    "      'name': 'disable_prior_visit_history',\n",
    "      'type': 'dataparam',\n",
    "      'values': [False, True],\n",
    "    },\n",
    "    {\n",
    "      'name': 'disable_difficulty_history',\n",
    "      'type': 'dataparam',\n",
    "      'values': [False, True],\n",
    "    },\n",
    "    {\n",
    "      'name': 'enable_current_difficulty',\n",
    "      'type': 'dataparam',\n",
    "      'values': [False, True],\n",
    "    },\n",
    "  ]\n",
    "  return output\n",
    "\n",
    "@memoize\n",
    "def get_parameter_name_to_info():\n",
    "  output = {}\n",
    "  for parameter_info in get_parameter_info_list():\n",
    "    parameter_name = parameter_info['name']\n",
    "    output[parameter_name] = parameter_info\n",
    "  return output\n",
    "\n",
    "def is_valid_parameter(parameter_name):\n",
    "  return parameter_name in get_parameter_name_to_info()\n",
    "\n",
    "def is_valid_parameter_value(parameter_name, parameter_value):\n",
    "  return parameter_value in get_parameter_values(parameter_name)\n",
    "\n",
    "def get_parameter_names():\n",
    "  return [x['name'] for x in get_parameter_info_list()]\n",
    "\n",
    "def get_parameter_info(parameter_name):\n",
    "  return get_parameter_name_to_info().get(parameter_name, None)\n",
    "\n",
    "def get_parameter_type(parameter_name):\n",
    "  return get_parameter_info(parameter_name)['type']\n",
    "\n",
    "def get_parameter_values(parameter_name):\n",
    "  return get_parameter_info(parameter_name)['values']\n",
    "\n",
    "def get_parameter_default(parameter_name):\n",
    "  return get_parameter_values(parameter_name)[0]\n",
    "\n",
    "def get_parameter_random_value(parameter_name):\n",
    "  values = get_parameter_values(parameter_name)\n",
    "  return random.choice(values)\n",
    "\n",
    "#   enabled_feature_list = parameters['enabled_feature_list']\n",
    "#   num_prior_entries = parameters.get('num_prior_entries', 10)\n",
    "#   sample_every_n_visits = parameters.get('sample_every_n_visits', 1)\n",
    "#   sample_difficulty_every_n_visits = parameters.get('sample_difficulty_every_n_visits', 1)\n",
    "#   disable_prior_visit_history = parameters.get('disable_prior_visit_history', False)\n",
    "#   disable_difficulty_history = parameters.get('disable_difficulty_history', False)\n",
    "#   enable_current_difficulty = parameters.get('enable_current_difficulty', False)\n",
    "\n",
    "# ['difficulty', 'time_of_day', 'day_of_week', 'domain_productivity', 'domain_category', 'initial_difficulty', 'languages']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_parameters(list_of_parameters_to_sample):\n",
    "  output = []\n",
    "  set_of_parameters_to_sample = set(list_of_parameters_to_sample)\n",
    "  for parameter_name in get_parameter_names():\n",
    "    if not is_valid_parameter(parameter_name):\n",
    "      print('invalid parameter ' + parameter_name)\n",
    "      continue\n",
    "    if parameter_name in set_of_parameters_to_sample:\n",
    "      val = get_parameter_random_value(parameter_name)\n",
    "    else:\n",
    "      val = get_parameter_default(parameter_name)\n",
    "    parameter_info = copy.copy(get_parameter_info(parameter_name))\n",
    "    parameter_info['value'] = val\n",
    "    output.append(parameter_info)\n",
    "  enabled_features_list = get_enabled_features_list_from_parameter_info_list(output)\n",
    "  num_features = get_num_features(enabled_features_list)\n",
    "  output.append({\n",
    "    'name': 'num_features',\n",
    "    'type': 'model',\n",
    "    'values': [num_features],\n",
    "    'value': num_features,\n",
    "  })\n",
    "  return output\n",
    "\n",
    "#print(sample_random_parameters(['difficulty']))\n",
    "\n",
    "def get_parameter_map_for_type(parameter_info_list, parameter_type):\n",
    "  output = {}\n",
    "  for parameter_info in parameter_info_list:\n",
    "    if parameter_info['type'] == parameter_type:\n",
    "      print(parameter_info)\n",
    "      val = parameter_info['value']\n",
    "      name = parameter_info['name']\n",
    "      output[name] = val\n",
    "  return output\n",
    "\n",
    "def get_enabled_features_list_from_parameter_info_list(parameter_info_list):\n",
    "  output = []\n",
    "  for parameter_info in parameter_info_list:\n",
    "    if parameter_info['type'] == 'feature':\n",
    "      val = parameter_info['value']\n",
    "      if val != True:\n",
    "        continue\n",
    "      name = parameter_info['name']\n",
    "      output.append(name)\n",
    "  return output\n",
    "\n",
    "\n",
    "def get_data_for_parameters(parameter_info_list):\n",
    "  print('running get_data_for_parameters')\n",
    "  dataparams = get_parameter_map_for_type(parameter_info_list, 'dataparam')\n",
    "  enabled_feature_list = get_enabled_features_list_from_parameter_info_list(parameter_info_list)\n",
    "  dataparams['enabled_feature_list'] = enabled_feature_list\n",
    "  print('running get_all_features_data')\n",
    "  all_features_data = get_all_features_data()\n",
    "  all_data_tensors = make_tensors_from_features(all_features_data, dataparams)\n",
    "  print('running split_into_train_dev_test')\n",
    "  return split_into_train_dev_test(all_data_tensors) # training,dev,test\n",
    "\n",
    "def get_model_for_parameter_map(model_params):\n",
    "  learning_rate = model_params['learning_rate']\n",
    "  model_name = model_params['model_name']\n",
    "  model_constructor = importlib.import_module('models.' + model_name).HLModel\n",
    "  model = model_constructor(model_params)\n",
    "  return model\n",
    "\n",
    "def get_model_for_parameters(parameter_info_list):\n",
    "  model_params = get_parameter_map_for_type(parameter_info_list, 'model')\n",
    "  #enabled_feature_list = get_enabled_features_list_from_parameter_info_list(parameter_info_list)\n",
    "  #num_features = get_num_features(enabled_feature_list)\n",
    "  return get_model_for_parameter_map(model_params)\n",
    "\n",
    "def get_parameter_value_for_info_list(parameter_info_list, parameter_name):\n",
    "  for parameter_info in parameter_info_list:\n",
    "    if parameter_info['name'] == parameter_name:\n",
    "      return parameter_info['value']\n",
    "\n",
    "#get_parameter_map_for_type(sample_random_parameters(['difficulty']), 'dataparam')\n",
    "#get_all_data_for_parameters(sample_random_parameters(['difficulty']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_random_parameters([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_model_for_parameters(sample_random_parameters([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_difficulty(tensor):\n",
    "  difficulty_idx = tensor[0].data.cpu().numpy()\n",
    "  return ['nothing', 'easy', 'medium', 'hard'][difficulty_idx]\n",
    "\n",
    "def prediction_to_difficulty(output):\n",
    "  top_n,top_i = output.topk(1)\n",
    "  category_i = top_i[0].item()\n",
    "  return ['nothing','easy','medium','hard'][category_i]\n",
    "\n",
    "def save_model(model, criterion, epoch, loss, filename):\n",
    "  torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': criterion.state_dict(),\n",
    "    'loss': loss,\n",
    "  }, filename)\n",
    "\n",
    "\n",
    "def train_transformer(model, criterion, category_tensor, line_tensor, learning_rate):\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # TODO: construct mask and lengths for batch size == 1\n",
    "    lengths = torch.tensor([line_tensor.size()[0]])\n",
    "    mask = torch.zeros(line_tensor.size()[0], line_tensor.size()[1], 1, dtype=torch.float)\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available() else\n",
    "                   torch.device('cpu'))\n",
    "    mask = mask.to(device)\n",
    "    line_tensor = line_tensor.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "    # END TODO\n",
    "#     print(line_tensor.size())\n",
    "#     print(category_tensor.size())\n",
    "#     print(lengths.size())\n",
    "#     print(mask.size())\n",
    "    output = model(line_tensor, lengths, mask)\n",
    "    # print(category_tensor.size())\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "def make_prediction(model, line_tensor):\n",
    "    line_tensor = line_tensor.permute(1,0,2)\n",
    "    #model.zero_grad()\n",
    "    \n",
    "    # TODO: construct mask and lengths for batch size == 1\n",
    "    lengths = torch.tensor([line_tensor.size()[0]])\n",
    "    mask = torch.zeros(line_tensor.size()[0], line_tensor.size()[1], 1, dtype=torch.float)\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available() else\n",
    "                   torch.device('cpu'))\n",
    "    mask = mask.to(device)\n",
    "    line_tensor = line_tensor.to(device)\n",
    "    #category_tensor = category_tensor.to(device)\n",
    "    # END TODO\n",
    "#     print(line_tensor.size())\n",
    "#     print(category_tensor.size())\n",
    "#     print(lengths.size())\n",
    "#     print(mask.size())\n",
    "    output = model(line_tensor, lengths, mask)\n",
    "    # print(category_tensor.size())\n",
    "    #loss = criterion(output, category_tensor)\n",
    "    #loss.backward()\n",
    "    return output\n",
    "\n",
    "def evaluate_model_on_dataset(model, dataset, prefix='dev_'):\n",
    "  confusion = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for item in dataset:\n",
    "    category_tensor = item['category']\n",
    "    feature_tensor = item['feature']\n",
    "    if feature_tensor.size()[0] == 0:\n",
    "        continue\n",
    "    difficulty = tensor_to_difficulty(category_tensor)\n",
    "    difficulty_idx = get_difficulty_idx(difficulty)\n",
    "    predicted_tensor = make_prediction(model, feature_tensor.cuda())\n",
    "    predicted_difficulty = prediction_to_difficulty(predicted_tensor)\n",
    "    predicted_difficulty_idx = get_difficulty_idx(predicted_difficulty)\n",
    "    if predicted_difficulty_idx == difficulty_idx:\n",
    "      correct += 1\n",
    "    total += 1\n",
    "    confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "  return {\n",
    "    prefix + 'correct': correct,\n",
    "    prefix + 'total': total,\n",
    "    prefix + 'confusion': confusion,\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import base64\n",
    "import arrow\n",
    "\n",
    "def convert_string_to_hash(word):\n",
    "    return hashlib.sha1(word.encode('utf-8')).hexdigest()\n",
    "\n",
    "def get_path_for_parameters(parameter_info_list):\n",
    "  output = []\n",
    "  for parameter_info in parameter_info_list:\n",
    "    name = parameter_info['name']\n",
    "    value = parameter_info['value']\n",
    "    output.append(\"'\".join([name, str(value)]))\n",
    "  return ' '.join(output)\n",
    "\n",
    "def train_one_epoch(model, criterion, learning_rate, train_data):\n",
    "  total_loss = 0\n",
    "  confusion = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for idx,item in enumerate(train_data):\n",
    "    category_tensor = item['category']\n",
    "    line_tensor = item['feature']\n",
    "    category = tensor_to_difficulty(item['category'])\n",
    "    category_i = get_difficulty_idx(category)\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output,loss = train_transformer(model, criterion, category_tensor, line_tensor.permute(1,0,2), learning_rate)\n",
    "    total_loss += loss\n",
    "    guess = prediction_to_difficulty(output)\n",
    "    guess_i = get_difficulty_idx(guess)\n",
    "    if guess_i == category_i:\n",
    "      correct += 1\n",
    "    confusion[category_i][guess_i] += 1\n",
    "    total += 1\n",
    "  return {\n",
    "    'train_loss': total_loss,\n",
    "    'train_correct': correct,\n",
    "    'train_total': total,\n",
    "    'train_confusion': confusion,\n",
    "  }\n",
    "\n",
    "def train_model_for_parameters(parameter_info_list, num_epochs=3):\n",
    "  base_path_full = get_path_for_parameters(parameter_info_list)\n",
    "  base_path = 'tm_' + convert_string_to_hash(base_path_full)\n",
    "  if path.exists(base_path):\n",
    "    # todo check status file and resume training\n",
    "    return\n",
    "  else:\n",
    "    os.mkdir(base_path)\n",
    "  json.dump({'base_path': base_path}, open('current.json', 'w'))\n",
    "  status_info = {\n",
    "    'status': 'training',\n",
    "    'epoch': 0,\n",
    "    'base_path': base_path,\n",
    "    'base_path_full': base_path_full,\n",
    "    'dataset_name': dataset_name,\n",
    "    'start_time': str(arrow.get()),\n",
    "    'start_timestamp': arrow.get().timestamp,\n",
    "  }\n",
    "  print(base_path)\n",
    "  json.dump(parameter_info_list, open(path.join(base_path, 'parameters.json'), 'w'))\n",
    "  model = get_model_for_parameters(parameter_info_list)\n",
    "  learning_rate = get_parameter_value_for_info_list(parameter_info_list, 'learning_rate')\n",
    "  train_data,dev_data,test_data = get_data_for_parameters(parameter_info_list)\n",
    "  criterion = nn.NLLLoss()\n",
    "  for epoch in range(1, 1 + num_epochs):\n",
    "    status_info['epoch'] = epoch\n",
    "    json.dump(status_info, open(path.join(base_path, 'status.json'), 'w'))\n",
    "    print(status_info)\n",
    "    epoch_start_time = str(arrow.get())\n",
    "    epoch_start_timestamp = arrow.get().timestamp\n",
    "    model_path = path.join(base_path, 'model_' + str(epoch) + '.pt')\n",
    "    if path.exists(model_path):\n",
    "      continue\n",
    "    train_info = train_one_epoch(model, criterion, learning_rate, train_data)\n",
    "    save_model(model, criterion, epoch, train_info['train_loss'], model_path)\n",
    "    dev_info = evaluate_model_on_dataset(model, dev_data, 'dev_')\n",
    "    test_info = evaluate_model_on_dataset(model, test_data, 'test_')\n",
    "    for k,v in dev_info.items():\n",
    "      train_info[k] = v\n",
    "    for k,v in test_info.items():\n",
    "      train_info[k] = v\n",
    "    train_info['epoch_start_time'] = epoch_start_time\n",
    "    train_info['epoch_start_timestamp'] = epoch_start_timestamp\n",
    "    train_info['epoch_end_time'] = str(arrow.get())\n",
    "    train_info['epoch_end_timestamp'] = arrow.get().timestamp\n",
    "    training_start_timestamp = arrow.get().timestamp\n",
    "    info_path = path.join(base_path, 'info_' + str(epoch) + '.json')\n",
    "    json.dump(train_info, open(info_path, 'w'))\n",
    "  status_info['status'] = 'done'\n",
    "  status_info['end_time'] = str(arrow.get())\n",
    "  status_info['end_timestamp'] = arrow.get().timestamp\n",
    "  json.dump(status_info, open(path.join(base_path, 'status.json'), 'w'))\n",
    "  print(status_info)\n",
    "\n",
    "#get_path_for_parameters(sample_random_parameters([]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  while True:\n",
    "    parameters = sample_random_parameters(['learning_rate', 'window_embed_size', 'num_prior_entries'])\n",
    "    train_model_for_parameters(parameters)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_string_to_hash('foobar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tm_52a58513923c54f3a174226adefb111c18571fbf\n",
      "{'name': 'model_name', 'type': 'model', 'values': ['selfattentionlstm'], 'value': 'selfattentionlstm'}\n",
      "{'name': 'criterion', 'type': 'model', 'values': ['NLLLoss'], 'value': 'NLLLoss'}\n",
      "{'name': 'learning_rate', 'type': 'model', 'values': [0.005, 0.05, 0.0005, 5e-05], 'value': 0.005}\n",
      "{'name': 'window_embed_size', 'type': 'model', 'values': [64, 128, 256, 512], 'value': 512}\n",
      "{'name': 'num_features', 'type': 'model', 'values': [277], 'value': 277}\n",
      "{'name': 'num_prior_entries', 'type': 'dataparam', 'values': [10, 20, 30, 40], 'value': 30}\n",
      "{'name': 'sample_every_n_visits', 'type': 'dataparam', 'values': [1], 'value': 1}\n",
      "{'name': 'sample_difficulty_every_n_visits', 'type': 'dataparam', 'values': [1], 'value': 1}\n",
      "{'name': 'disable_prior_visit_history', 'type': 'dataparam', 'values': [False, True], 'value': False}\n",
      "{'name': 'disable_difficulty_history', 'type': 'dataparam', 'values': [False, True], 'value': False}\n",
      "{'name': 'enable_current_difficulty', 'type': 'dataparam', 'values': [False, True], 'value': False}\n",
      "{'status': 'training', 'epoch': 1, 'base_path': 'tm_52a58513923c54f3a174226adefb111c18571fbf', 'base_path_full': \"dataset_name'2019_04_01 model_name'selfattentionlstm criterion'NLLLoss learning_rate'0.005 window_embed_size'512 difficulty'True time_of_day'True day_of_week'True domain_productivity'True domain_category'True initial_difficulty'True languages'True num_prior_entries'30 sample_every_n_visits'1 sample_difficulty_every_n_visits'1 disable_prior_visit_history'False disable_difficulty_history'False enable_current_difficulty'False num_features'277\", 'dataset_name': '2019_04_01', 'start_time': '2019-04-04T06:56:26.943772+00:00', 'start_timestamp': 1554360986}\n",
      "{'status': 'training', 'epoch': 2, 'base_path': 'tm_52a58513923c54f3a174226adefb111c18571fbf', 'base_path_full': \"dataset_name'2019_04_01 model_name'selfattentionlstm criterion'NLLLoss learning_rate'0.005 window_embed_size'512 difficulty'True time_of_day'True day_of_week'True domain_productivity'True domain_category'True initial_difficulty'True languages'True num_prior_entries'30 sample_every_n_visits'1 sample_difficulty_every_n_visits'1 disable_prior_visit_history'False disable_difficulty_history'False enable_current_difficulty'False num_features'277\", 'dataset_name': '2019_04_01', 'start_time': '2019-04-04T06:56:26.943772+00:00', 'start_timestamp': 1554360986}\n",
      "{'status': 'training', 'epoch': 3, 'base_path': 'tm_52a58513923c54f3a174226adefb111c18571fbf', 'base_path_full': \"dataset_name'2019_04_01 model_name'selfattentionlstm criterion'NLLLoss learning_rate'0.005 window_embed_size'512 difficulty'True time_of_day'True day_of_week'True domain_productivity'True domain_category'True initial_difficulty'True languages'True num_prior_entries'30 sample_every_n_visits'1 sample_difficulty_every_n_visits'1 disable_prior_visit_history'False disable_difficulty_history'False enable_current_difficulty'False num_features'277\", 'dataset_name': '2019_04_01', 'start_time': '2019-04-04T06:56:26.943772+00:00', 'start_timestamp': 1554360986}\n"
     ]
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
