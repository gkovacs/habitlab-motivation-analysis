{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook train_model.ipynb to python\n",
      "train_model.py:330: error: Too few arguments for \"iterateTrainingData\"\n"
     ]
    }
   ],
   "source": [
    "# noexport\n",
    "\n",
    "!typech train_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['difficulty', 'time_of_day', 'day_of_week', 'domain_productivity', 'domain_category', 'initial_difficulty', 'languages']\n"
     ]
    }
   ],
   "source": [
    "from mkdata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_selfattentionlstm import SelfAttentionLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_data = get_all_features_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_all_features,dev_data_all_features,test_data_all_features = split_into_train_dev_test(all_features_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateTrainingData(training_data):\n",
    "  output = []\n",
    "  for data in training_data:\n",
    "    category = tensor_to_difficulty(data['category'])\n",
    "    #yield category,data['category'],data['feature']\n",
    "    output.append((category,data['category'],data['feature']))\n",
    "  np.random.shuffle(output)\n",
    "  return output\n",
    "\n",
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(rnn, criterion, epoch, loss, filename):\n",
    "  torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': rnn.state_dict(),\n",
    "    'optimizer_state_dict': criterion.state_dict(),\n",
    "    'loss': loss,\n",
    "  }, filename)\n",
    "\n",
    "#print(criterion.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3])\n",
      "hard\n"
     ]
    }
   ],
   "source": [
    "def tensor_to_difficulty(tensor):\n",
    "  difficulty_idx = tensor[0].data.cpu().numpy()\n",
    "  return ['nothing', 'easy', 'medium', 'hard'][difficulty_idx]\n",
    "\n",
    "def categoryFromOutput(output):\n",
    "  top_n,top_i = output.topk(1)\n",
    "  category_i = top_i[0].item()\n",
    "  return ['nothing','easy','medium','hard'][category_i],category_i\n",
    "\n",
    "#print(output.topk(1))\n",
    "#print(categoryFromOutput(output))\n",
    "\n",
    "print(make_tensor_from_chosen_difficulty('hard'))\n",
    "print(tensor_to_difficulty((make_tensor_from_chosen_difficulty('hard'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTransformer(category_tensor, line_tensor):\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    # TODO: construct mask and lengths for batch size == 1\n",
    "    lengths = torch.tensor([line_tensor.size()[0]])\n",
    "    mask = torch.zeros(line_tensor.size()[0], line_tensor.size()[1], 1, dtype=torch.float)\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available() else\n",
    "                   torch.device('cpu'))\n",
    "    mask = mask.to(device)\n",
    "    line_tensor = line_tensor.to(device)\n",
    "    category_tensor = category_tensor.to(device)\n",
    "    # END TODO\n",
    "#     print(line_tensor.size())\n",
    "#     print(category_tensor.size())\n",
    "#     print(lengths.size())\n",
    "#     print(mask.size())\n",
    "    output = model(line_tensor, lengths, mask)\n",
    "    # print(category_tensor.size())\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = get_feature_names()\n",
    "num_features = get_num_features(feature_names)\n",
    "num_prior_entries = 10\n",
    "enable_current_difficulty = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making training data for sample_difficulty_every_n_visits 84\n"
     ]
    }
   ],
   "source": [
    "n_iters = 100\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "\n",
    "for sample_difficulty_every_n_visits in range(1, 1001):\n",
    "  epoch = 1\n",
    "  outfile = 'model_attention_nohistory_fulldata_nhidden512_sample_difficulty_every_n_visits_' + str(sample_difficulty_every_n_visits) + '_v10_epoch' + str(epoch) + '.pt'\n",
    "  if os.path.exists(outfile):\n",
    "    continue\n",
    "  print('making training data for sample_difficulty_every_n_visits', sample_difficulty_every_n_visits)\n",
    "  training_data = make_tensors_from_features(training_data_all_features, {\n",
    "    'enabled_feature_list': feature_names,\n",
    "    'num_prior_entries': num_prior_entries,\n",
    "    'enable_current_difficulty': enable_current_difficulty,\n",
    "    'sample_difficulty_every_n_visits': sample_difficulty_every_n_visits,\n",
    "  })\n",
    "  model = SelfAttentionLSTM({'word_embed_size': num_features, 'window_embed_size': 128})\n",
    "  all_training_items = iterateTrainingData(training_data)\n",
    "  print('sample_difficulty_every_n_visits', sample_difficulty_every_n_visits)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = trainTransformer(category_tensor, line_tensor.permute(1,0,2))\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(epoch, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)\n",
    "  save_model(model, criterion, epoch, current_loss, outfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "\n",
    "for sample_every_n_visits in range(1, 11):\n",
    "  print('making training data for sample_every_n_visits', sample_every_n_visits)\n",
    "  training_data = make_tensors_from_features(training_data_all_features[:int(len(training_data_all_features) / 10)], {\n",
    "    'enabled_feature_list': feature_names,\n",
    "    'num_prior_entries': num_prior_entries,\n",
    "    'enable_current_difficulty': enable_current_difficulty,\n",
    "    'sample_every_n_visits': sample_every_n_visits,\n",
    "  })\n",
    "  model = SelfAttentionLSTM({'word_embed_size': num_features, 'window_embed_size': 128})\n",
    "  epoch = 1\n",
    "  all_training_items = iterateTrainingData(training_data)\n",
    "  print('sample_every_n_visits', sample_every_n_visits)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = trainTransformer(category_tensor, line_tensor.permute(1,0,2))\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(epoch, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)\n",
    "  save_model(model, criterion, epoch, current_loss, 'model_attention_nohistory_fractiondata10_nhidden512_sample_every_n_visits_' + str(sample_every_n_visits) + '_v10_epoch' + str(epoch) + '.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "disable_difficulty_history = True\n",
    "sample_every_n_visits = 1\n",
    "\n",
    "#for sample_every_n_visits in range(1, 11):\n",
    "if True:\n",
    "  print('making training data for disable_difficulty_history', disable_difficulty_history)\n",
    "  training_data = make_tensors_from_features(training_data_all_features[:int(len(training_data_all_features) / 10)], {\n",
    "    'enabled_feature_list': feature_names,\n",
    "    'num_prior_entries': num_prior_entries,\n",
    "    'enable_current_difficulty': enable_current_difficulty,\n",
    "    'sample_every_n_visits': sample_every_n_visits,\n",
    "    'disable_difficulty_history': disable_difficulty_history,\n",
    "  })\n",
    "  model = SelfAttentionLSTM({'word_embed_size': num_features, 'window_embed_size': 128})\n",
    "  epoch = 1\n",
    "  all_training_items = iterateTrainingData(training_data)\n",
    "  print('sample_every_n_visits', sample_every_n_visits)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = trainTransformer(category_tensor, line_tensor.permute(1,0,2))\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(epoch, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)\n",
    "  save_model(model, criterion, epoch, current_loss, 'model_attention_nohistory_fractiondata10_nhidden512_disable_difficulty_history_v10_epoch' + str(epoch) + '.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "disable_prior_visit_history = True\n",
    "\n",
    "#for sample_every_n_visits in range(1, 11):\n",
    "if True:\n",
    "  print('making training data for disable_prior_visit_history', disable_prior_visit_history)\n",
    "  training_data = make_tensors_from_features(training_data_all_features[:int(len(training_data_all_features) / 10)], {\n",
    "    'enabled_feature_list': feature_names,\n",
    "    'num_prior_entries': num_prior_entries,\n",
    "    'enable_current_difficulty': enable_current_difficulty,\n",
    "    'sample_every_n_visits': sample_every_n_visits,\n",
    "    'disable_prior_visit_history': disable_prior_visit_history,\n",
    "  })\n",
    "  model = SelfAttentionLSTM({'word_embed_size': num_features, 'window_embed_size': 128})\n",
    "  epoch = 1\n",
    "  all_training_items = iterateTrainingData(training_data)\n",
    "  print('sample_every_n_visits', sample_every_n_visits)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = trainTransformer(category_tensor, line_tensor.permute(1,0,2))\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(epoch, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)\n",
    "  save_model(model, criterion, epoch, current_loss, 'model_attention_nohistory_fractiondata10_nhidden512_disable_prior_visit_history_v10_epoch' + str(epoch) + '.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, n_iters + 1):\n",
    "  all_training_items = iterateTrainingData()\n",
    "  print('iteration', epoch)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = trainTransformer(category_tensor, line_tensor.permute(1,0,2))\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(epoch, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)\n",
    "  save_model(model, criterion, epoch, current_loss, 'model_attention_nohistory_nhidden512_v10_epoch' + str(epoch) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
