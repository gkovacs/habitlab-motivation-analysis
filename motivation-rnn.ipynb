{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.2 (default, Dec 25 2018, 03:50:46) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import libmotivation\n",
    "reload(libmotivation)\n",
    "from libmotivation import *\n",
    "import mkdata\n",
    "reload(mkdata)\n",
    "from mkdata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_list = get_users_with_choose_difficulty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_difficulty_items = {}\n",
    "for user in user_list:\n",
    "  user_to_difficulty_items[user] = get_choose_difficulty_items_for_user(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arrow\n",
    "from dateutil import tz\n",
    "\n",
    "#timezone_list = list(timezone_set)\n",
    "#print(timezone_list)\n",
    "#print(list(map(to_hours_and_minutes, timezone_list)))\n",
    "\n",
    "def parse_timezone_offset(datestr):\n",
    "  date_parts = datestr.split(' ')\n",
    "  for x in date_parts:\n",
    "    if 'GMT' in x:\n",
    "      output = (x.replace('GMT', ''))\n",
    "      #timezone_set.add(output)\n",
    "      return output\n",
    "  raise Exception(datestr)\n",
    "\n",
    "def to_hours_and_minutes(offset):\n",
    "  if len(offset) != 5:\n",
    "    raise Exception(offset)\n",
    "  sign = offset[0]\n",
    "  hours = int(offset[1:3])\n",
    "  minutes = int(offset[3:])\n",
    "  if sign == '+':\n",
    "    return hours,minutes\n",
    "  if sign == '-':\n",
    "    return -hours,-minutes\n",
    "  raise Exception(offset)\n",
    "\n",
    "def adjust_timestamp_to_timezone_offset(timestamp, offset):\n",
    "  hours,minutes = to_hours_and_minutes(offset)\n",
    "  #print(hours, minutes)\n",
    "  ar = arrow.get(timestamp / 1000.0)\n",
    "  return ar.shift(hours=hours, minutes=minutes)\n",
    "\n",
    "def get_time_adjusted_for_timezone(timestamp, datestr):\n",
    "  offset = parse_timezone_offset(datestr)\n",
    "  return adjust_timestamp_to_timezone_offset(timestamp, offset)\n",
    "\n",
    "#print(adjust_timestamp_to_timezone(1548789649098.0,'-0600')) # 'Tue Jan 29 2019 13:20:49 GMT-0600 (Central Standard Time)',\n",
    "print(get_time_adjusted_for_timezone(1548789649098.0, 'Tue Jan 29 2019 13:20:49 GMT-0600 (Central Standard Time)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "timezone_set = set()\n",
    "\n",
    "\n",
    "\n",
    "timestamp_server = 1544629175088.0\n",
    "timestamp_local = 1544629174633.0\n",
    "#(timestamp_local - timestamp_server) / (1000*3600)\n",
    "#arrow.get(timestamp_local / 1000)\n",
    "#timestamp\n",
    "#arrow.get('Wed Dec 12 2018 07:39:34 GMT-0800 (Pacific Standard Time)')\n",
    "print(parse_timezone_offset('Wed Dec 12 2018 07:39:34 GMT-0800 (Pacific Standard Time)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in user_list:\n",
    "  difficulty_items = user_to_difficulty_items[user]\n",
    "  for x in difficulty_items:\n",
    "    parse_timezone_offset(x['localtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in user_list[1000:]:\n",
    "  difficulty_items = user_to_difficulty_items[user]\n",
    "  if len(difficulty_items) > 0:\n",
    "    print(difficulty_items[0])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_most_common_difficulty_for_user('8d2c9eb27dee2dc85bca705b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_most_common_difficulty_overall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctness_of_naive_overall_strategy(user):\n",
    "  output = {'correct': 0, 'incorrect': 0}\n",
    "  difficulty_counts = get_choose_difficulty_counts_for_user(user)\n",
    "  most_common = get_most_common_difficulty_overall()\n",
    "  if most_common == None:\n",
    "    return None\n",
    "  output['total'] = sum(difficulty_counts.values())\n",
    "  if output['total'] == 0:\n",
    "    return None\n",
    "  output['correct'] = difficulty_counts.get(most_common, 0)\n",
    "  output['incorrect'] = output['total'] - output['correct']\n",
    "  output['accuracy'] = output['correct'] / output['total']\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correctness_of_naive_peruser_strategy(user):\n",
    "  output = {'correct': 0, 'incorrect': 0}\n",
    "  difficulty_counts = get_choose_difficulty_counts_for_user(user)\n",
    "  most_common = get_most_common_difficulty_for_user(user)\n",
    "  if most_common == None:\n",
    "    return None\n",
    "  output['correct'] = difficulty_counts[most_common]\n",
    "  output['total'] = sum(difficulty_counts.values())\n",
    "  output['incorrect'] = output['total'] - output['correct']\n",
    "  output['accuracy'] = output['correct'] / output['total']\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctnes_of_naive_per_user = []\n",
    "for user in user_list:\n",
    "  correctness_info = get_correctness_of_naive_peruser_strategy(user)\n",
    "  if correctness_info == None:\n",
    "    continue\n",
    "  accuracy = correctness_info['accuracy']\n",
    "  correctnes_of_naive_per_user.append(accuracy)\n",
    "\n",
    "print(np.median(correctnes_of_naive_per_user))\n",
    "print(np.mean(correctnes_of_naive_per_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctnes_of_naive_per_user = []\n",
    "for user in user_list:\n",
    "  correctness_info = get_correctness_of_naive_overall_strategy(user)\n",
    "  if correctness_info == None:\n",
    "    continue\n",
    "  accuracy = correctness_info['accuracy']\n",
    "  correctnes_of_naive_per_user.append(accuracy)\n",
    "\n",
    "print(np.median(correctnes_of_naive_per_user))\n",
    "print(np.mean(correctnes_of_naive_per_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_unnormalized = 0\n",
    "num_total_unnormalized = 0\n",
    "for user in user_list:\n",
    "  correctness_info = get_correctness_of_naive_overall_strategy(user)\n",
    "  if correctness_info == None:\n",
    "    continue\n",
    "  num_correct_unnormalized += correctness_info['correct']\n",
    "  num_total_unnormalized += correctness_info['total']\n",
    "  #accuracy = correctness_info['accuracy']\n",
    "  #correctnes_of_naive_per_user.append(accuracy)\n",
    "\n",
    "#print(np.median(correctnes_of_naive_per_user))\n",
    "#print(np.mean(correctnes_of_naive_per_user_unnormalized))\n",
    "print(num_correct_unnormalized / num_total_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_correct_unnormalized = 0\n",
    "num_total_unnormalized = 0\n",
    "for user in user_list:\n",
    "  correctness_info = get_correctness_of_naive_peruser_strategy(user)\n",
    "  if correctness_info == None:\n",
    "    continue\n",
    "  num_correct_unnormalized += correctness_info['correct']\n",
    "  num_total_unnormalized += correctness_info['total']\n",
    "  #accuracy = correctness_info['accuracy']\n",
    "  #correctnes_of_naive_per_user.append(accuracy)\n",
    "\n",
    "#print(np.median(correctnes_of_naive_per_user))\n",
    "#print(np.mean(correctnes_of_naive_per_user_unnormalized))\n",
    "print(num_correct_unnormalized / num_total_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features = 4 # the choice at each step. nothing, easy, medium, hard\n",
    "#n_features = 5\n",
    "n_features = 4 + 4 + 7 + 5 + 69 + 5 + get_num_languages()\n",
    "n_categories = 4 # the number of categories we are categorizing among. nothing, easy, medium, hard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_to_idx = {\n",
    "  'nothing': 0,\n",
    "  'easy': 1,\n",
    "  'medium': 2,\n",
    "  'hard': 3,\n",
    "}\n",
    "\n",
    "# def difficulty_to_idx(difficulty):\n",
    "#   return {\n",
    "#     'nothing': 0,\n",
    "#     'easy': 1,\n",
    "#     'medium': 2,\n",
    "#     'hard': 3,\n",
    "#   }[difficulty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "domain_to_productivity = json.load(open('domain_to_productivity.json'))\n",
    "domain_to_category = json.load(open('domain_to_category.json'))\n",
    "\n",
    "category_list = list(set(domain_to_category.values()))\n",
    "\n",
    "domain_to_productivity_idx = {}\n",
    "domain_to_category_idx = {}\n",
    "for domain,productivity in domain_to_productivity.items():\n",
    "  # should we do something about domains where productivity = 0?\n",
    "  domain_to_productivity_idx[domain] = productivity + 2\n",
    "domain_to_category_idx = {}\n",
    "for domain,category in domain_to_category.items():\n",
    "  # should we do something about domains where category = ''?\n",
    "  category_idx = category_list.index(category)\n",
    "  domain_to_category_idx[domain] = category_idx\n",
    "print(len(set(domain_to_category_idx.values())))\n",
    "print(len(set(domain_to_productivity_idx.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_to_domain(url):\n",
    "  #url='https://www.hello.org/bye/'\n",
    "  domain=url.split('//')[-1].split('/')[0]\n",
    "  #print (domain)\n",
    "  return domain\n",
    "\n",
    "print(url_to_domain('https://www.hello.org/bye/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_tensors_for_user(user):\n",
    "  features = make_features_for_user(user)\n",
    "  return make_tensors_from_features(features)\n",
    "\n",
    "# def make_tensor_from_prior_difficulties_list(prior_difficulties):\n",
    "#   tensor = torch.zeros(len(prior_difficulties), 1, n_features)\n",
    "#   for idx,difficulty in enumerate(prior_difficulties):\n",
    "#     difficulty_idx = difficulty_to_idx[difficulty]\n",
    "#     tensor[idx][0][difficulty_idx] = 1\n",
    "#   return tensor\n",
    "\n",
    "def make_tensor_from_chosen_difficulty(chosen_difficulty):\n",
    "  difficulty_idx = difficulty_to_idx[chosen_difficulty]\n",
    "  tensor = torch.tensor([difficulty_idx], dtype=torch.long)\n",
    "  return tensor\n",
    "\n",
    "def make_tensors_from_features_v1(features):\n",
    "  output = []\n",
    "  for feature in features:\n",
    "    chosen_difficulty = feature['difficulty']\n",
    "    prior_difficulties = feature['prior_difficulties']\n",
    "    user = feature['user']\n",
    "    category_tensor = make_tensor_from_chosen_difficulty(chosen_difficulty)\n",
    "    feature_tensor = torch.zeros(len(prior_difficulties), 1, n_features) # n_features = 4 in this version\n",
    "    for idx,difficulty in enumerate(prior_difficulties):\n",
    "      difficulty_idx = difficulty_to_idx[difficulty]\n",
    "      feature_tensor[idx][0][difficulty_idx] = 1\n",
    "    #feature_tensor = make_tensor_from_prior_difficulties_list(prior_difficulties)\n",
    "    output.append({'user': user, 'chosen_difficulty': chosen_difficulty, 'category': category_tensor, 'feature': feature_tensor})\n",
    "  return output\n",
    "\n",
    "def make_tensors_from_features_v2(features):\n",
    "  output = []\n",
    "  for feature in features:\n",
    "    hour = feature['arrow_time'].hour\n",
    "    chosen_difficulty = feature['difficulty']\n",
    "    prior_difficulties = feature['prior_difficulties']\n",
    "    user = feature['user']\n",
    "    category_tensor = make_tensor_from_chosen_difficulty(chosen_difficulty)\n",
    "    feature_tensor = torch.zeros(len(prior_difficulties), 1, n_features) # n_features = 1 in this version\n",
    "    for idx,difficulty in enumerate(prior_difficulties):\n",
    "      difficulty_idx = difficulty_to_idx[difficulty]\n",
    "      feature_tensor[idx][0][0] = difficulty_idx\n",
    "    output.append({'user': user, 'chosen_difficulty': chosen_difficulty, 'category': category_tensor, 'feature': feature_tensor})\n",
    "  return output\n",
    "\n",
    "def hour_to_hour_idx_4cat(hour): # hour: 0 to 24\n",
    "  if 0 <= hour <= 6:\n",
    "    return 0\n",
    "  if 6 < hour <= 12:\n",
    "    return 1\n",
    "  if 12 < hour <= 18:\n",
    "    return 2\n",
    "  if 18 < hour <= 24:\n",
    "    return 3\n",
    "  raise Exception(hour)\n",
    "\n",
    "def make_tensors_from_features_v3(features):\n",
    "  output = []\n",
    "  for feature in features:\n",
    "    hour = feature['arrow_time'].hour\n",
    "    hour_idx = hour_to_hour_idx_4cat(hour)\n",
    "    weekday_idx = feature['arrow_time'].weekday()\n",
    "    chosen_difficulty = feature['difficulty']\n",
    "    prior_difficulties = feature['prior_difficulties']\n",
    "    user = feature['user']\n",
    "    category_tensor = make_tensor_from_chosen_difficulty(chosen_difficulty)\n",
    "    feature_tensor = torch.zeros(len(prior_difficulties), 1, n_features) # n_features = 15 in this version\n",
    "    for idx,difficulty in enumerate(prior_difficulties):\n",
    "      difficulty_idx = difficulty_to_idx[difficulty]\n",
    "      feature_tensor[idx][0][difficulty_idx] = 1\n",
    "      feature_tensor[idx][0][4 + hour_idx] = 1\n",
    "      feature_tensor[idx][0][8 + weekday_idx] = 1\n",
    "    output.append({'user': user, 'chosen_difficulty': chosen_difficulty, 'category': category_tensor, 'feature': feature_tensor})\n",
    "  return output\n",
    "\n",
    "def make_tensors_from_features_v4(features):\n",
    "  output = []\n",
    "  for feature in features:\n",
    "    url = feature['url']\n",
    "    domain = url_to_domain(url)\n",
    "    have_productivity_idx = False\n",
    "    if domain in domain_to_productivity_idx:\n",
    "      domain_productivity_idx = domain_to_productivity_idx[domain]\n",
    "      have_productivity_idx = True\n",
    "    else:\n",
    "      domain_productivity_idx = None\n",
    "    have_category_idx = False\n",
    "    if domain in domain_to_category_idx:\n",
    "      domain_category_idx = domain_to_productivity_idx[domain]\n",
    "      have_category_idx = True\n",
    "    else:\n",
    "      domain_category_idx = None\n",
    "    hour = feature['arrow_time'].hour\n",
    "    hour_idx = hour_to_hour_idx_4cat(hour)\n",
    "    weekday_idx = feature['arrow_time'].weekday()\n",
    "    chosen_difficulty = feature['difficulty']\n",
    "    prior_difficulties = feature['prior_difficulties']\n",
    "    user = feature['user']\n",
    "    category_tensor = make_tensor_from_chosen_difficulty(chosen_difficulty)\n",
    "    feature_tensor = torch.zeros(len(prior_difficulties), 1, n_features) # n_features = 15 in this version\n",
    "    for idx,difficulty in enumerate(prior_difficulties):\n",
    "      difficulty_idx = difficulty_to_idx[difficulty]\n",
    "      feature_tensor[idx][0][difficulty_idx] = 1\n",
    "      feature_tensor[idx][0][4 + hour_idx] = 1\n",
    "      feature_tensor[idx][0][4 + 4 + weekday_idx] = 1\n",
    "      if have_productivity_idx:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + domain_productivity_idx] = 1\n",
    "      if have_category_idx:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + 5 + domain_category_idx] = 1\n",
    "    output.append({'user': user, 'chosen_difficulty': chosen_difficulty, 'category': category_tensor, 'feature': feature_tensor})\n",
    "  return output\n",
    "\n",
    "def get_domain_features_from_url(url):\n",
    "  domain = url_to_domain(url)\n",
    "  have_productivity_idx = False\n",
    "  if domain in domain_to_productivity_idx:\n",
    "    domain_productivity_idx = domain_to_productivity_idx[domain]\n",
    "    have_productivity_idx = True\n",
    "  else:\n",
    "    domain_productivity_idx = None\n",
    "  have_category_idx = False\n",
    "  if domain in domain_to_category_idx:\n",
    "    domain_category_idx = domain_to_productivity_idx[domain]\n",
    "    have_category_idx = True\n",
    "  else:\n",
    "    domain_category_idx = None\n",
    "  return domain_productivity_idx,have_productivity_idx,domain_category_idx,have_category_idx\n",
    "\n",
    "def get_time_features_from_arrow(arrow_time):\n",
    "  hour = arrow_time.hour\n",
    "  hour_idx = hour_to_hour_idx_4cat(hour)\n",
    "  weekday_idx = arrow_time.weekday()\n",
    "  return hour_idx,weekday_idx\n",
    "\n",
    "def make_tensors_from_features_v5(features): # this one cheats because we have the reference information included\n",
    "  output = []\n",
    "  for feature in features:\n",
    "    url = feature['url']\n",
    "    domain_productivity_idx,have_productivity_idx,domain_category_idx,have_category_idx = get_domain_features_from_url(url)\n",
    "    hour_idx,weekday_idx = get_time_features_from_arrow(feature['arrow_time'])\n",
    "    chosen_difficulty = feature['difficulty']\n",
    "    prior_difficulties = feature['prior_difficulties']\n",
    "    user = feature['user']\n",
    "    category_tensor = make_tensor_from_chosen_difficulty(chosen_difficulty)\n",
    "    feature_tensor = torch.zeros(len(prior_difficulties)+1, 1, n_features) # n_features = 15 in this version\n",
    "    # features for current timestep\n",
    "    idx = len(prior_difficulties)\n",
    "    difficulty = feature['difficulty']\n",
    "    difficulty_idx = difficulty_to_idx[difficulty]\n",
    "    #if len(prior_difficulties) > 0:\n",
    "    #  feature_tensor[idx][0][difficulty_to_idx[prior_difficulties[-1]]] = 1\n",
    "    #feature_tensor[idx][0][difficulty_idx] = 1 # this is an impossible feature. see whether it learns corectly\n",
    "    feature_tensor[idx][0][4 + hour_idx] = 1\n",
    "    feature_tensor[idx][0][4 + 4 + weekday_idx] = 1\n",
    "    if have_productivity_idx:\n",
    "      feature_tensor[idx][0][4 + 4 + 7 + domain_productivity_idx] = 1\n",
    "    if have_category_idx:\n",
    "      feature_tensor[idx][0][4 + 4 + 7 + 5 + domain_category_idx] = 1\n",
    "    # features for previous timesteps\n",
    "    for idx,difficulty in enumerate(prior_difficulties):\n",
    "      difficulty_idx = difficulty_to_idx[difficulty]\n",
    "      #feature_tensor[idx][0][difficulty_idx] = 1\n",
    "      hour_idx,weekday_idx = get_time_features_from_arrow(feature['prior_arrow_times'][idx])\n",
    "      domain_productivity_idx,have_productivity_idx,domain_category_idx,have_category_idx = get_domain_features_from_url(feature['prior_urls'][idx])\n",
    "      feature_tensor[idx][0][4 + hour_idx] = 1\n",
    "      feature_tensor[idx][0][4 + 4 + weekday_idx] = 1\n",
    "      if have_productivity_idx:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + domain_productivity_idx] = 1\n",
    "      if have_category_idx:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + 5 + domain_category_idx] = 1\n",
    "    output.append({'user': user, 'chosen_difficulty': chosen_difficulty, 'category': category_tensor, 'feature': feature_tensor})\n",
    "  return output\n",
    "\n",
    "def make_tensors_from_features_v6(features): # this one cheats because we have the reference information included\n",
    "  output = []\n",
    "  for feature in features:\n",
    "    url = feature['url']\n",
    "    domain_productivity_idx,have_productivity_idx,domain_category_idx,have_category_idx = get_domain_features_from_url(url)\n",
    "    hour_idx,weekday_idx = get_time_features_from_arrow(feature['arrow_time'])\n",
    "    chosen_difficulty = feature['difficulty']\n",
    "    language_indexes = convert_language_list_to_language_indexes(feature['languages'])\n",
    "    prior_difficulties = feature['prior_difficulties']\n",
    "    user = feature['user']\n",
    "    category_tensor = make_tensor_from_chosen_difficulty(chosen_difficulty)\n",
    "    feature_tensor = torch.zeros(len(prior_difficulties)+1, 1, n_features) # n_features = 15 in this version\n",
    "    # features for current timestep\n",
    "    idx = len(prior_difficulties)\n",
    "    difficulty = feature['difficulty']\n",
    "    difficulty_idx = difficulty_to_idx[difficulty]\n",
    "    #if len(prior_difficulties) > 0:\n",
    "    #  feature_tensor[idx][0][difficulty_to_idx[prior_difficulties[-1]]] = 1\n",
    "    #feature_tensor[idx][0][difficulty_idx] = 1 # this is an impossible feature. see whether it learns corectly\n",
    "    feature_tensor[idx][0][4 + 4 + 7 + 5 + 69 + difficulty_to_idx[feature['initial_difficulty']]] = 1\n",
    "    for language_index in language_indexes:\n",
    "      feature_tensor[idx][0][4 + 4 + 7 + 5 + 69 + 5 + language_index] = 1\n",
    "    feature_tensor[idx][0][4 + hour_idx] = 1\n",
    "    feature_tensor[idx][0][4 + 4 + weekday_idx] = 1\n",
    "    if have_productivity_idx:\n",
    "      feature_tensor[idx][0][4 + 4 + 7 + domain_productivity_idx] = 1\n",
    "    if have_category_idx:\n",
    "      feature_tensor[idx][0][4 + 4 + 7 + 5 + domain_category_idx] = 1\n",
    "    # features for previous timesteps\n",
    "    for idx,difficulty in enumerate(prior_difficulties):\n",
    "      difficulty_idx = difficulty_to_idx[difficulty]\n",
    "      #feature_tensor[idx][0][difficulty_idx] = 1\n",
    "      hour_idx,weekday_idx = get_time_features_from_arrow(feature['prior_arrow_times'][idx])\n",
    "      domain_productivity_idx,have_productivity_idx,domain_category_idx,have_category_idx = get_domain_features_from_url(feature['prior_urls'][idx])\n",
    "      feature_tensor[idx][0][4 + hour_idx] = 1\n",
    "      feature_tensor[idx][0][4 + 4 + weekday_idx] = 1\n",
    "      if have_productivity_idx:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + domain_productivity_idx] = 1\n",
    "      if have_category_idx:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + 5 + domain_category_idx] = 1\n",
    "      feature_tensor[idx][0][4 + 4 + 7 + 5 + 69 + difficulty_to_idx[feature['initial_difficulty']]] = 1\n",
    "      for language_index in language_indexes:\n",
    "        feature_tensor[idx][0][4 + 4 + 7 + 5 + 69 + 5 + language_index] = 1\n",
    "    output.append({'user': user, 'chosen_difficulty': chosen_difficulty, 'category': category_tensor, 'feature': feature_tensor})\n",
    "  return output\n",
    "\n",
    "make_tensors_from_features = make_tensors_from_features_v6\n",
    "\n",
    "history_length = 10\n",
    "\n",
    "def make_features_for_user(user):\n",
    "  output = []\n",
    "  difficulty_items = get_choose_difficulty_items_for_user(user)\n",
    "  prior_difficulties = []\n",
    "  prior_urls = []\n",
    "  prior_arrow_times = []\n",
    "  languages = get_languages_for_user(user)\n",
    "  initial_difficulty = get_initial_difficulty_for_user(user)\n",
    "  if initial_difficulty == None:\n",
    "    return []\n",
    "  for item in difficulty_items:\n",
    "    if 'is_random' in item and item['is_random'] == True:\n",
    "      continue\n",
    "    if 'type' not in item:\n",
    "      continue\n",
    "    if item['type'] != 'action':\n",
    "      continue\n",
    "    if 'difficulty' not in item:\n",
    "      continue\n",
    "    difficulty = item['difficulty']\n",
    "    url = item['url']\n",
    "    arrow_time = get_time_adjusted_for_timezone(item['timestamp_local'], item['localtime'])\n",
    "    output.append({'url': url, 'user': user, 'initial_difficulty': initial_difficulty, 'languages': languages, 'difficulty': difficulty, 'arrow_time': arrow_time, 'prior_urls': prior_urls[:], 'prior_difficulties': prior_difficulties[:], 'prior_arrow_times': prior_arrow_times[:]})\n",
    "    prior_difficulties.append(difficulty)\n",
    "    prior_urls.append(url)\n",
    "    prior_arrow_times.append(arrow_time)\n",
    "    if len(prior_difficulties) > history_length:\n",
    "      prior_difficulties = prior_difficulties[-history_length:]\n",
    "      prior_urls = prior_urls[-history_length:]\n",
    "      prior_arrow_times  = prior_arrow_times[-history_length:]\n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arrow.get().shift(hours=-8, days=-4).weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(make_tensors_from_features([{'url': 'facebook.com', 'user': 'foobar', 'difficulty': 'easy', 'prior_difficulties': [], 'arrow_time': arrow.get().shift(hours=-8)}]))\n",
    "#print(make_tensors_from_features([{'url': 'facebook.com', 'user': 'foobar', 'difficulty': 'easy', 'prior_difficulties': ['easy', 'nothing'], 'arrow_time': arrow.get().shift(hours=-8)}]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for user in user_list:\n",
    "  print(user)\n",
    "  tensors = make_tensors_for_user(user)\n",
    "  for tensor in tensors:\n",
    "    all_data.append(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_difficulty(tensor):\n",
    "  difficulty_idx = tensor[0].data.cpu().numpy()\n",
    "  return ['nothing', 'easy', 'medium', 'hard'][difficulty_idx]\n",
    "\n",
    "print(make_tensor_from_chosen_difficulty('hard'))\n",
    "print(tensor_to_difficulty((make_tensor_from_chosen_difficulty('hard'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = all_data[:int(math.floor(len(all_data)*9/10))]\n",
    "dev_and_test_data = all_data[len(training_data):]\n",
    "dev_data = dev_and_test_data[:int(math.floor(len(dev_and_test_data)/2))]\n",
    "test_data = dev_and_test_data[len(dev_data):]\n",
    "\n",
    "def iterateTrainingData():\n",
    "  output = []\n",
    "  for data in training_data:\n",
    "    category = tensor_to_difficulty(data['category'])\n",
    "    #yield category,data['category'],data['feature']\n",
    "    output.append((category,data['category'],data['feature']))\n",
    "  np.random.shuffle(output)\n",
    "  return output\n",
    "\n",
    "def randomTrainingExample():\n",
    "  data = random.choice(training_data)\n",
    "  category = tensor_to_difficulty(data['category'])\n",
    "  return category,data['category'],data['feature']\n",
    "\n",
    "#def makeDataTensor(lines_of_data):\n",
    "#  output = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(randomTrainingExample())\n",
    "category,category_tensor,line_tensor = iterateTrainingData()[4]\n",
    "print(line_tensor.size())\n",
    "#print('line tensor size dimension 1 is')\n",
    "#print(line_tensor.size()[0])\n",
    "#train(category_tensor.cuda(), line_tensor.cuda())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden().cuda()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN1, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 128\n",
    "\n",
    "class RNN2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN2, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        print('pre softmax output size is')\n",
    "        print(output.size())\n",
    "        output = self.softmax(output)\n",
    "        print('post softmax output size is')\n",
    "        print(output.size())\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 256\n",
    "\n",
    "rnn = RNN2(n_features, n_hidden, n_categories).cuda()\n",
    "\n",
    "class RNN3(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, dropout=0.2):\n",
    "        super(RNN3, self).__init__()\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.nlayers = num_layers\n",
    "        self.nhid = hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        #self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        #self.softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self, input, hidden):\n",
    "      #print('input.shape in forward is')\n",
    "      #print(input.size())\n",
    "      #print('hidden[0].shape in forward is')\n",
    "      #print(hidden[0].size())\n",
    "      #print('hidden[1].shape in forward is')\n",
    "      #print(hidden[1].size())\n",
    "      input = input.view((input.size(0), 1, input.size(1)))\n",
    "      output,hidden = self.rnn(input, hidden)\n",
    "      #print('pre softmax output size is')\n",
    "      #print(output.size())\n",
    "      output = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "      output = self.softmax(output)\n",
    "      #print('post softmax output size is')\n",
    "      #print(output.size())\n",
    "      return output, hidden\n",
    "\n",
    "    def initHidden(self, bsz=1):\n",
    "        return (torch.zeros(2, 1, 256).cuda(), torch.zeros(2, 1, 256).cuda())\n",
    "        #weight = next(self.parameters())\n",
    "        #return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "        #       weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "    \n",
    "    #def initHidden(self):\n",
    "    #    return torch.zeros(10, 2, 1, self.hidden_size)\n",
    "\n",
    "n_hidden = 256\n",
    "\n",
    "rnn3 = RNN3(n_features, n_hidden, n_categories).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.tied).to(device)\n",
    "# LSTM 33278 200 200 2 0.2 False\n",
    "# rnn_type = args.model='LSTM'\n",
    "# ntoken = ntokens=33278 (number of tokens. presumably the number of words in the english language)\n",
    "# ninp = args.emsize=200 (size of word embeddings)\n",
    "# nhid = args.nhid=200 (number of hidden units per layer)\n",
    "# nlayers = args.nlayers=2 (number of layers)\n",
    "# dropout = args.dropout=0.2 (dropout applied to layers (0 = no dropout))\n",
    "# tie_weights = args.tied=False (tie the word embedding and softmax weights)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        #self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #print('input is')\n",
    "        #print(input.size()) # torch.Size([35, 20]) # 35 = sequence length, 20 = batch size\n",
    "        #emb = self.drop(self.encoder(input))\n",
    "        #print('emb is')\n",
    "        #print(emb.size()) # torch.Size([35, 20, 200]) # 35 = sequence length, 20 = batch size, 200 = embedding size\n",
    "        emb = input\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        return output,hidden\n",
    "        #output = self.drop(output)\n",
    "        #decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        #return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid), weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "\n",
    "rnn2 = RNNModel(rnn_type='LSTM', ntoken=4, ninp=4, nhid=200, nlayers=2, dropout=0, tie_weights=False)\n",
    "\n",
    "# def makeIntoTensor(data_list):\n",
    "#   output = {}\n",
    "#   line_to_user = {}\n",
    "  \n",
    "#   for idx,item in enumerate(training_data):\n",
    "#     print(idx)\n",
    "#     print(item)\n",
    "#     print(item.size())\n",
    "#     break\n",
    "\n",
    "# def makeTrainingData():\n",
    "#   return makeIntoTensor(training_data)\n",
    "  \n",
    "\n",
    "# makeTrainingData() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(rnn, criterion, epoch, loss, filename):\n",
    "  torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': rnn.state_dict(),\n",
    "    'optimizer_state_dict': criterion.state_dict(),\n",
    "    'loss': loss,\n",
    "  }, filename)\n",
    "\n",
    "#print(criterion.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "  top_n,top_i = output.topk(1)\n",
    "  category_i = top_i[0].item()\n",
    "  return ['nothing','easy','medium','hard'][category_i],category_i\n",
    "#print(output.topk(1))\n",
    "#print(categoryFromOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnn = RNN3(n_features, n_hidden, n_categories, dropout=0.2).cuda()\n",
    "\n",
    "#learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train2(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    hidden = repackage_hidden(hidden)\n",
    "    #hidden = hidden.cuda()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "n_iters = 500\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "all_losses = []\n",
    "\n",
    "rnn.train()\n",
    "all_training_items = iterateTrainingData()\n",
    "for epoch in range(1, n_iters + 1):\n",
    "  print('iteration', epoch)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = train2(category_tensor.cuda(), line_tensor.cuda())\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(epoch, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if epoch % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)\n",
    "  save_model(rnn, criterion, epoch, current_loss, 'model_rnn_v9_epoch' + str(epoch) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.train()\n",
    "#all_training_items = iterateTrainingData()\n",
    "for iter in range(1, n_iters + 1):\n",
    "  print('iteration', iter)\n",
    "  current_loss = 0\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    #print(line_tensor.size())\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = train2(category_tensor.cuda(), line_tensor.cuda())\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      #pass\n",
    "      print(iter, ':', idx, '/', len(all_training_items))\n",
    "      # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "        #print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "    # Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    #current_loss = 0\n",
    "  #rnn.zero_grad()\n",
    "  all_losses.append(current_loss / plot_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "n_iters = 100\n",
    "print_every = 1\n",
    "plot_every = 1\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "  #category,category_tensor, line_tensor = randomTrainingExample()\n",
    "  #if True:\n",
    "  all_training_items = iterateTrainingData()\n",
    "  for idx,training_item in enumerate(all_training_items):\n",
    "    (category,category_tensor, line_tensor) = training_item\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output, loss = train(category_tensor.cuda(), line_tensor.cuda())\n",
    "    current_loss += loss\n",
    "    if idx % 1000 == 0:\n",
    "      print(iter, ':', idx, '/', len(all_training_items))\n",
    "\n",
    "  # Print iter number, loss, name and guess\n",
    "  if iter % print_every == 0:\n",
    "      guess, guess_i = categoryFromOutput(output)\n",
    "      correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "      print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "  # Add current loss avg to list of losses\n",
    "  if iter % plot_every == 0:\n",
    "      all_losses.append(current_loss / plot_every)\n",
    "      current_loss = 0\n",
    "\n",
    "# for iter in range(1, n_iters + 1):\n",
    "#     category,category_tensor, line_tensor = randomTrainingExample()\n",
    "#     if line_tensor.size()[0] == 0:\n",
    "#       continue\n",
    "#     output, loss = train(category_tensor.cuda(), line_tensor.cuda())\n",
    "#     current_loss += loss\n",
    "\n",
    "#     # Print iter number, loss, name and guess\n",
    "#     if iter % print_every == 0:\n",
    "#         guess, guess_i = categoryFromOutput(output)\n",
    "#         correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "#         print('%d %d%% (%s) %.4f / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, guess, correct))\n",
    "\n",
    "#     # Add current loss avg to list of losses\n",
    "#     if iter % plot_every == 0:\n",
    "#         all_losses.append(current_loss / plot_every)\n",
    "#         current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(all_losses[:10])\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(all_losses[:10])\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = ['nothing', 'easy', 'medium', 'hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate3(model, line_tensor):\n",
    "    model.eval()\n",
    "    hidden = model.initHidden() #.cuda()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = model(line_tensor[i], hidden)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(line_tensor):\n",
    "    rnn.eval()\n",
    "    hidden = rnn.initHidden() #.cuda()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of correct guesses in a confusion matrix\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "n_confusion = 10000\n",
    "\n",
    "\n",
    "\n",
    "# Go through a bunch of examples and record which are correctly guessed\n",
    "for i in range(n_confusion):\n",
    "    category, category_tensor, line_tensor = randomTrainingExample()\n",
    "    if line_tensor.size()[0] == 0:\n",
    "      continue\n",
    "    output = evaluate(line_tensor.cuda())\n",
    "    guess, guess_i = categoryFromOutput(output)\n",
    "    category_i = difficulty_to_idx[category]\n",
    "    confusion[category_i][guess_i] += 1\n",
    "\n",
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(model, dataset):\n",
    "  num_correct = 0\n",
    "  num_incorrect = 0\n",
    "  for item in dataset:\n",
    "    category_tensor = item['category']\n",
    "    feature_tensor = item['feature']\n",
    "    if feature_tensor.size()[0] == 0:\n",
    "        continue\n",
    "    difficulty = tensor_to_difficulty(category_tensor)\n",
    "    difficulty_idx = difficulty_to_idx[difficulty]\n",
    "    predicted_tensor = evaluate3(model, feature_tensor.cuda())\n",
    "    predicted_difficulty,predicted_difficulty_idx = categoryFromOutput(predicted_tensor)\n",
    "    if predicted_difficulty_idx == difficulty_idx:\n",
    "      num_correct += 1\n",
    "    else:\n",
    "      num_incorrect += 1\n",
    "    confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "    #print(predicted_difficulty)\n",
    "    #print(difficulty)\n",
    "    #break\n",
    "\n",
    "  print('num correct: ' + str(num_correct))\n",
    "  print('num incorrect: ' + str(num_incorrect))\n",
    "  print('percentage correct: ' + str(num_correct / (num_correct + num_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion = torch.zeros(n_categories, n_categories)\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "\n",
    "for item in training_data:\n",
    "  category_tensor = item['category']\n",
    "  feature_tensor = item['feature']\n",
    "  if feature_tensor.size()[0] == 0:\n",
    "      continue\n",
    "  difficulty = tensor_to_difficulty(category_tensor)\n",
    "  difficulty_idx = difficulty_to_idx[difficulty]\n",
    "  predicted_tensor = evaluate(feature_tensor.cuda())\n",
    "  predicted_difficulty,predicted_difficulty_idx = categoryFromOutput(predicted_tensor)\n",
    "  if predicted_difficulty_idx == difficulty_idx:\n",
    "    num_correct += 1\n",
    "  else:\n",
    "    num_incorrect += 1\n",
    "  confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "  #print(predicted_difficulty)\n",
    "  #print(difficulty)\n",
    "  #break\n",
    "\n",
    "print('num correct: ' + str(num_correct))\n",
    "print('num incorrect: ' + str(num_incorrect))\n",
    "print('percentage correct: ' + str(num_correct / (num_correct + num_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "while True:\n",
    "  epoch += 1\n",
    "  model_filename = 'model_rnn_v9_epoch' + str(epoch) + '.pt'\n",
    "  if not os.path.isfile(model_filename):\n",
    "    break\n",
    "  print(model_filename)\n",
    "  #model = SelfAttentionLSTM()\n",
    "  model = RNN3(n_features, n_hidden, n_categories, dropout=0.2).cuda()\n",
    "  model_state = torch.load(model_filename)\n",
    "  model.load_state_dict(model_state['model_state_dict'])\n",
    "  model.eval()\n",
    "  evaluate_model_on_dataset(model, dev_data)\n",
    "  #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "while True:\n",
    "  epoch += 1\n",
    "  model_filename = 'model_rnn_v9_epoch' + str(epoch) + '.pt'\n",
    "  if not os.path.isfile(model_filename):\n",
    "    break\n",
    "  print(model_filename)\n",
    "  #model = SelfAttentionLSTM()\n",
    "  model = RNN3(n_features, n_hidden, n_categories, dropout=0.2).cuda()\n",
    "  model_state = torch.load(model_filename)\n",
    "  model.load_state_dict(model_state['model_state_dict'])\n",
    "  model.eval()\n",
    "  evaluate_model_on_dataset(model, training_data)\n",
    "  #break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion = torch.zeros(n_categories, n_categories)\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "\n",
    "for item in dev_data:\n",
    "  category_tensor = item['category']\n",
    "  feature_tensor = item['feature']\n",
    "  if feature_tensor.size()[0] == 0:\n",
    "      continue\n",
    "  difficulty = tensor_to_difficulty(category_tensor)\n",
    "  difficulty_idx = difficulty_to_idx[difficulty]\n",
    "  predicted_tensor = evaluate(feature_tensor.cuda())\n",
    "  predicted_difficulty,predicted_difficulty_idx = categoryFromOutput(predicted_tensor)\n",
    "  if predicted_difficulty_idx == difficulty_idx:\n",
    "    num_correct += 1\n",
    "  else:\n",
    "    num_incorrect += 1\n",
    "  confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "  #print(predicted_difficulty)\n",
    "  #print(difficulty)\n",
    "  #break\n",
    "\n",
    "print('num correct: ' + str(num_correct))\n",
    "print('num incorrect: ' + str(num_incorrect))\n",
    "print('percentage correct: ' + str(num_correct / (num_correct + num_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize by dividing every row by its sum\n",
    "for i in range(n_categories):\n",
    "    confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "# Set up plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(confusion.numpy())\n",
    "fig.colorbar(cax)\n",
    "\n",
    "# Set up axes\n",
    "ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "# Force label at every tick\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion = torch.zeros(n_categories, n_categories)\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "\n",
    "for item in dev_data:\n",
    "  user = item['user']\n",
    "  category_tensor = item['category']\n",
    "  feature_tensor = item['feature']\n",
    "  if feature_tensor.size()[0] == 0:\n",
    "      continue\n",
    "  difficulty = tensor_to_difficulty(category_tensor)\n",
    "  difficulty_idx = difficulty_to_idx[difficulty]\n",
    "  predicted_difficulty = get_most_common_difficulty_for_user(user)\n",
    "  predicted_difficulty_idx = difficulty_to_idx[predicted_difficulty]\n",
    "  predicted_tensor = make_tensor_from_chosen_difficulty(predicted_difficulty)\n",
    "  #predicted_tensor = evaluate(feature_tensor.cuda())\n",
    "  #predicted_difficulty,predicted_difficulty_idx = categoryFromOutput(predicted_tensor)\n",
    "  if predicted_difficulty_idx == difficulty_idx:\n",
    "    num_correct += 1\n",
    "  else:\n",
    "    num_incorrect += 1\n",
    "  confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "  #print(predicted_difficulty)\n",
    "  #print(difficulty)\n",
    "  #break\n",
    "\n",
    "print('num correct: ' + str(num_correct))\n",
    "print('num incorrect: ' + str(num_incorrect))\n",
    "print('percentage correct: ' + str(num_correct / (num_correct + num_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_recent_difficulty(tensor):\n",
    "  difficulty_arr = list(tensor[-1][0].data.cpu().numpy())\n",
    "  max_idx = difficulty_arr.index(1)\n",
    "  print(len(difficulty_arr))\n",
    "  print(max_idx)\n",
    "  return ['nothing', 'easy', 'medium', 'hard'][max_idx]\n",
    "\n",
    "#confusion = torch.zeros(n_categories, n_categories)\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "\n",
    "for item in dev_data:\n",
    "  user = item['user']\n",
    "  category_tensor = item['category']\n",
    "  feature_tensor = item['feature']\n",
    "  if feature_tensor.size()[0] == 0:\n",
    "      continue\n",
    "  difficulty = tensor_to_difficulty(category_tensor)\n",
    "  difficulty_idx = difficulty_to_idx[difficulty]\n",
    "  predicted_difficulty = get_most_recent_difficulty(feature_tensor)\n",
    "  predicted_difficulty_idx = difficulty_to_idx[predicted_difficulty]\n",
    "  predicted_tensor = make_tensor_from_chosen_difficulty(predicted_difficulty)\n",
    "  #predicted_tensor = evaluate(feature_tensor.cuda())\n",
    "  #predicted_difficulty,predicted_difficulty_idx = categoryFromOutput(predicted_tensor)\n",
    "  if predicted_difficulty_idx == difficulty_idx:\n",
    "    num_correct += 1\n",
    "  else:\n",
    "    num_incorrect += 1\n",
    "  confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "  #print(predicted_difficulty)\n",
    "  #print(difficulty)\n",
    "  #break\n",
    "\n",
    "print('num correct: ' + str(num_correct))\n",
    "print('num incorrect: ' + str(num_incorrect))\n",
    "print('percentage correct: ' + str(num_correct / (num_correct + num_incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion = torch.zeros(n_categories, n_categories)\n",
    "num_correct = 0\n",
    "num_incorrect = 0\n",
    "confusion = torch.zeros(n_categories, n_categories)\n",
    "\n",
    "for item in dev_data:\n",
    "  user = item['user']\n",
    "  category_tensor = item['category']\n",
    "  feature_tensor = item['feature']\n",
    "  if feature_tensor.size()[0] == 0:\n",
    "      continue\n",
    "  difficulty = tensor_to_difficulty(category_tensor)\n",
    "  difficulty_idx = difficulty_to_idx[difficulty]\n",
    "  #predicted_difficulty = get_most_common_difficulty_for_user(user)\n",
    "  predicted_difficulty = get_most_common_difficulty_overall()\n",
    "  predicted_difficulty_idx = difficulty_to_idx[predicted_difficulty]\n",
    "  predicted_tensor = make_tensor_from_chosen_difficulty(predicted_difficulty)\n",
    "  #predicted_tensor = evaluate(feature_tensor.cuda())\n",
    "  #predicted_difficulty,predicted_difficulty_idx = categoryFromOutput(predicted_tensor)\n",
    "  if predicted_difficulty_idx == difficulty_idx:\n",
    "    num_correct += 1\n",
    "  else:\n",
    "    num_incorrect += 1\n",
    "  confusion[difficulty_idx][predicted_difficulty_idx] += 1\n",
    "  #print(predicted_difficulty)\n",
    "  #print(difficulty)\n",
    "  #break\n",
    "\n",
    "print('num correct: ' + str(num_correct))\n",
    "print('num incorrect: ' + str(num_incorrect))\n",
    "print('percentage correct: ' + str(num_correct / (num_correct + num_incorrect)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
